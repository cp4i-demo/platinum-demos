{"componentChunkName":"component---src-pages-300-watson-aiops-proactively-assure-application-performance-demo-script-mdx","path":"/300-watson-aiops-proactively-assure-application-performance/demo-script/","result":{"pageContext":{"frontmatter":{"title":"Proactively Assure Application Performance 300-level live demo","description":"Rapid incident resolution 300-level live demo","tabs":["Demo preparation","Demo script"]},"relativePagePath":"/300-watson-aiops-proactively-assure-application-performance/demo-script.mdx","titleType":"page","MdxNode":{"id":"4856f1d8-6be6-5c66-beb6-f4947209e35d","children":[],"parent":"fe1d2731-d6d1-5861-8350-20667b602050","internal":{"content":"---\ntitle: Proactively Assure Application Performance\n  300-level live demo\ndescription: Rapid incident resolution 300-level live demo\ntabs: [ 'Demo preparation', 'Demo script']\n---\n\nexport const Title = () => (\n  <span>\n    Proactively Assure Application Performance <br/> 300-level live demo\n  </span> );\n\n![banner](./images/Incident-Resolution-300-Script.jpg)\n\n<span id=\"place1\"></span>\n\n<details>\n<summary>Introduction</summary>\n<br/>\n\nIn this demo we will:<br/><br/>First, use Instana to discover the various business applications and get a quick view of their overall health.<br/><br/>Then, we’ll see how Turbonomic leverages the information from Instana and other workload controllers to provide recommendations for application performance & resource optimization.<br/><br/>Lastly, we’ll see how Turbonomic recommendations can be ***automatically*** executed via defined automation policies that effectively remove the need for human intervention. After the requisite level of TRUST is established, these policies enable the system to proactively take remedial actions when appropriate.\n<br/>\n(Printer-ready PDF of demo script <a href=\"./NEW PDF HERE.pdf\" target=\"_blank\" rel=\"noreferrer\">here</a>)\n\n<br/><br/>\n\n**[Go to top](#place1)**\n\n</details>\n\n<details>\n\n<summary>1 - Visualizing your environment</summary>\n\n<br/>\n\n| **1.1** | **Navigate to the Anomaly Generator and input sources** |\n| :--- | :--- |\n| **Actions** | Navigate to **Instana** <br/><br/>Click **Applications** in left-hand sidebar menu |\n| **Narration** | We’re a retail company with multiple online storefronts representing each of our diversified brands. For example, we sell robots via our Robot Shop-branded ecommerce site.<br/><br/>**This is a list of all our business applications, which we are monitoring with Instana. Instana provides simplicity with its full-stack discoverability capabilities and an ability to trace every request end-to-end. This makes it easy to track each application’s health and performance. We can quickly see key information for each business application, including number of calls, latency, and the erroneous call rate.<br/><br/>Some users have been complaining about slowness and unpredictable performance issues with the Robot Shop Application recently. |\n| **Screenshot** | <br/> ![IMAGE PENDING](./images/quote-app.png) |\n\n| **1.2** | **Verify which hosts the app runs on** |\n| :--- | :--- |\n| **Actions** | Click on **Robot Shop**<br/><br/>Then click the **Stack** button to see the dropdown<br/><br/>Click **Infrastructure** to see the hosts that Robot Shop runs on |\n| **Narration** | To accelerate our strategic digital transformation initiatives, our company has embraced cloud native architecture. Applications are build based on microservices and deployed to a Kubernetes-based infrastructure. Many lines of business deploy to and share a common infrastructure. This means that a node might have parts of multiple and different business applications on it. When one application suddenly has an increase in workload, it can affect the others as well. Since not all applications are built reliably, some may have unpredictable load patterns - especially during promotion events.<br/><br/>For example, our Robot Shop application has components running on two different nodes or “hosts” as they are known in Instana. |\n| **Screenshot** | <br/> ![IMAGE PENDING](./images/quote-app.png) |\n\n| **1.3** | **Shared Deployment Infrastructure** |\n| :--- | :--- |\n| **Actions** | Click on the **worker2** host<br/><br/>On worker2’s summary screen, click the **Stack** button to see the dropdown<br/><br/>Click **Application** to see the other applications with components on worker2 |\n| **Narration** | Looking at one of the hosts, worker2, we can see that it has components of four different applications running on it. It’s easy to see that if there’s a sudden or unexpected increase in demand on Robot Shop, it could affect the Quote of the Daya app as well - or vice versa. |\n| **Screenshot** | <br/> ![IMAGE PENDING](./images/quote-app.png) |\n\n<br/><br/>\n\n**[Go to top](#place1)**\n\n</details>\n\n<details>\n\n<summary>2 - Gaining full stack visibility into your hybrid and multicloud environments</summary>\n\n<br/>\n\n| **2.1** | **Get the big picture** |\n| :--- | :--- |\n| **Action** | Navigate to the **Turbonomic** Home Page – **Global View**<br/><br/>Examine **Global Supply Chain**<br/><br/>Hoover over the **Supply Chain**<br/><br/>Click **Virtual Machines Entity** in Supply Chain |\n| **Narration** | Before we take a deeper look at the Robot Shop app specifically, let’s take a step back and understand our application infrastructure more wholistically. Older applications were deployed to isolated environments, but modern apps are deployed to a highly shared, increasingly dense virtualized hybrid and multi-cloud environment. Hence getting a complete view is essential as resourcing decisions in one area will naturally impact another.<br/><br/>Turbonomic ingests configuration and operational data from various targets like Instana and combines all the information it has discovered into a common data model and represents it as a Supply Chain.<br/><br/>The Supply Chain provides the big picture, serving as an informative graphical organizer of the various entities in the IT estate. It also shows the implicit dynamic relationships, all the way from the application to its underlying infrastructure dependencies.<br/><br/>This quick visualization provids incredibly valuable information to the IT Ops teams. It helps to bridge the communication gap that often exists between the application owners and the infrastructure teams.<br/><br/>Okay, let’s explore the Supply Chain in a bit more detail here:<br/><br/>• Starting right at the top you have the Business Applications. Note this environment consists of 63 business applications<br/>• There are 544 key business transactions and 1000 services that are part of this estate.<br/>• The top three layers are called LOGICAL layers – they do not consume physical resources<br/>• The services are deployed to physical resources like JVMs or Operating Systems processes that run in a container or VM.<br/>• Going down the Supply Chain you can see the components that build up the lower layers of the application stack – including Database Servers, Storage Volumes and Hosts among others.<br/>• Going even further down the stack you can see how the virtualized resources could be part of. Virtual Data Center or part of a public Cloud region and availability zone.<br/>• Each circle in the supply chain represents a logical or physical entity.<br/>• The number inside is the count of the number of entities of this type<br/>• The ring of each entry indicates the percentage of pending actions<br/>• Green shows percentage of entities that have NO pending actions<br/>• Yellow shows entities that have efficiency pending actions<br/>• Red shows entities that have critical performance pending actions<br/>• Hover over the entity type to get actual counts of pending actions<br/>• The arrow from one entry to another indicates the flow of resource consumption<br/>• For example –a VM consumes resources from hosts and storage<br/>• You can see this environment consists of 972 Virtual Machines.<br/>• Clicking on an Entity Type will bring you to a detailed view into those entities.<br/><br/>Internally, the supply chain model is key to the optimization analytics that Turbonomic performs on the environment. It models each entity in the environment as part of an economic model – where some resources are buyers, and some are sellers. Each resource is allocated a budget and Turbonomic continually assesses the supply and demand of resources. If something is in short supply, its price goes up. This dynamic adjustment using a market-based approach means that Turbonomic can find the most efficient use of resources given performance needs and constraints. |\n| **Screenshot** | <br/> ![IMAGE TO COME](./images/incident-title.png) |\n<br/><br/>\n\n**[Go to top](#place1)**\n\n</details>\n\n\nCURRENT\n\n<details>\n\n<summary>3 - SCOPE the analysis to Robot Shop</summary>\n\n<br/>\n\n| **3.1** | **SCOPE to the Robot Shop Supply Chain** |\n| :--- | :--- |\n| **Action** | Scroll to the **Relevant events** section of the story |\n| **Narration** | We need to find out where the issue began so we can prevent it from causing cascading failures across the components of the application.<br/><br/>Instead of having to go to Prometheus or another tool to look at the alerts, we can see them right here from the notification. Watson AIOps has determined that these events are related, and provides an explanation for how it determined the relationships. We can see that there are two groups of events based on related resources and the timing of the events.<br/><br/>\n| **Screenshot** | <br/> ![Relevant events](./images/relevant-events.png) |\n| **Actions** | Click on the **View relevant events** button at the bottom of the notification<br/><br/>A pop-up will appear with the grouped events<br/>\n| **Narration** | We can inspect the grouped events right here, without searching for them in another tool.<br/><br/>It looks like the memory and CPU on the Rating service increased significantly. This is causing a significant slow-down in response time on both the Rating and Web services.<br/><br/>Based on this information, it seems that the Rating service is the source of the issue. But let’s get a bit more detail – this time from the log files.<br/><br/>\n| **Screenshots** | <br/> ![View relevant events](./images/view-relevant-events.png) <br/>![Relevant events alerts](./images/relevant-events-alerts.png) |\n| **Action** | Scroll down past the alerts to show **Log anomaly** |\n| **Narration** | Instead of needing to go to Kibana and manually sort through the hundreds or thousands of log entries that come in every minute, Watson AIOps has found several anomalies in the log files and presented them here. It trains on the log files of the application when it’s operating normally, and continually monitors for deviations from that baseline.<br/><br/>We can see that the anomalies are occurring on the Rating service, which fits with what we saw in the alerts. |\n| **Screenshot** | <br/> ![Rating service](./images/rating-service.png) |\n| **Action** | Click on **Show more** |\n| **Narration** | Watson AIOps gives us additional context on the anomaly. In this case, the “unknown_error” anomaly is telling us that Watson AIOps has never seen this type of log before (hence the “unknown”) and that the log message indicates there is some type of error. Watson AIOps is not only looking at the statistical frequency of the type of log, but it is also using Natural Language Processing to analyze the content of the log message to give additional context (in this case that there’s likely an error).<br/><br/>Watson AIOps also explains why the anomaly was flagged. It expected to see zero (0) of this type of log, but it actually saw four (4).<br/><br/>Now we know that there is an unfamiliar log coming from the Rating service, and it’s indicating an error.<br/><br/>This further reinforces what we saw with the alerts -  it looks like the Rating service is likely the root cause of this problem.<br/><br/>\n| **Screenshots** | <br/> ![anomaly](./images/anomaly.png) <br/>![anomaly 2](./images/log-anomaly-1.png) |\n| **Action** | For the 'Unknown_error' anomaly, click on **Preview logs** |\n| **Narration** | We can preview the log messages that caused Watson AIOps to find the anomaly. |\n| **Screenshots** | <br/> ![Preview logs](./images/preview-logs.png) <br/> ![Log preview](./images/log-preview.png) |\n\n**[Go to top](#place1)**\n\n</details>\n\n<details>\n\n<summary>4 - Towards self-healing and automating your action FINALIZE</summary>\n\n<br/>\n\n| **4.1** | **Watson AIOps searches similar incidents for recommendations** |\n| :--- | :--- |\n| **Actions** | Close the log anomaly pop-up<br/><br/>Click on the **Search similar incidents** button<br/><br/>Type 'rating' into the search box |\n| **Narration** | Now that we understand a bit more about what’s going on, we need to focus on our main goal: resolving the incident as quickly as possible.<br/><br/>Watson AIOps also brings in similar incidents and information regarding how they were resolved. This enhances operational efficiency by leveraging institutional knowledge that may be time consuming to find otherwise.<br/><br/>Since the alerts seem to point to the Rating service as having the memory and CPU increases, we’ll search for incidents that happened with that service. <br/><br/>Watson AIOps found two similar incidents. The first one seems like what we’re experiencing: Rating service is overheating and slowing. Using Natural Language Processing, Watson AIOps  went through the comments on the incident and highlighted key relevant information for us, in this case that the incident was resolved by running a runbook.<br/><br/>If we can use the same runbook to resolve the current incident it will make our job that much easier and restore service even faster! |\n| **Screenshots** | <br/> ![Search similar incidents](./images/search-similar-incidents.png) <br/>![Search similar incidents - Rating](./images/search-similar-incidents-rating.png) <br/>![Search results](./images/search-results.png) |\n| **Actions** | Click on the link for the first similar incident **Rating service overheating and slowing**<br/><br/>It will open a window in GitLab<br/><br/>If needed, sort comments **newest first** |\n| **Narration** | The description for this incident sounds a lot like what we are observing right now. The Rating service is stressed, with increases in CPU, memory, and latency. It’s also affecting the Web service.<br/><br/>The comment says there is a runbook that resolved this issue. This is very useful information that may have taken a long time to find out just by searching or asking colleagues. Watson AIOps helped us find this info very quickly. |\n| **Screenshots** | <br/> ![Newest first](./images/newest-first.png) |\n\n**[Go to top](#place1)**\n\n</details>\n\n<details>\n\n<summary>5 - Fixing the problem and restoring service</summary>\n\n<br/>\n\n| **5.1** | **View Event Manager to determine the root cause and fix the problem** |\n| :--- | :--- |\n| **Actions** | Navigate to **Event Manager** (formally known as NOI)<br/><br/>Click on **Events** in the sidebar menu<br/><br/>The top event should show a red circle (**critical**)<br/><br/>Click on the **Down** arrow to expand it |\n| **Narration** | This is the Event Manager component of Watson AIOps. Here’s the event group that we’re working on. There are four events grouped together – these are the same ones we previewed earlier from the Slack notification. There’s additional detail here, including the likelihood of each event being the root cause of the overall issue and if there is a runbook associated with the event.<br/><br/>There are three events that occurred on the Rating service. They’re all rated as 99% likely to be the root cause - meaning that the Rating service is the source of the problem.<br/><br/>It also shows us that there’s a runbook associated with these events, which is what was mentioned in the GitLab incident comment. |\n| **Screenshots** | <br/> ![Events1](./images/events1.png) <br/> ![Events2](./images/events2.png) |\n| **Actions** | Click on the second row to select it (**Summary** column: **Rating service Mem usage very high**)<br/><br/>Click on **Execute runbook**<br/><br/>Click on **Start runbook**<br/><br/>Click on **Run** |\n| **Narration** | Now we’ll execute the runbook that should resolve the underlying failure of the Rating service. That should fix the problem. |\n| **Screenshots** | <br/> ![Events3](./images/events3.png) <br/>![runbook1](./images/runbook1.png) <br/> ![Runbook2](./images/runbook2.png) |\n| **Actions** | Navigate to **Instana**<br/><br/>Click on the **Summary** tab<br/><br/>Set the time period to **last 15 minutes**, and click on the **Live** button (you will be showing the latency before, during, and after the incident) |\n| **Narration** | To check that the runbook is fixing the problem, we can look at the health metrics in Instana. We’ll set the time period so we can see data from when the incident began until now.<br/><br/>We can see that the latency of the application started increasing when the problem began. Now that we’ve run the runbook, latency is going back down. This confirms that the runbook was successful. |\n| **Screenshot** | <br/> ![runbook success](./images/runbook-success.png) |\n\n**[Go to top](#place1)**\n\n</details>\n\n<details>\n\n<summary>Summary</summary>\n\n<br/>\nUsing Turbonomic and Instana, we were able to easily identify changes needed to maintain quality of service, as well as improve efficiency, for our multiple applications which included components running on AWS and OpenShift.\n\nFirst, using Instana, we were able to discover the business applications and monitor their health.\n\nThen, using Turbonomic, we got specific recommendations on how to assure performance and improve efficiency of our operations.\n<br/><br/>\n\n**[Go to top](#place1)**\n\n</details>\n","type":"Mdx","contentDigest":"bd2a8cf96f2c29c8fc6beb1aa9226d65","owner":"gatsby-plugin-mdx","counter":1609},"frontmatter":{"title":"Proactively Assure Application Performance 300-level live demo","description":"Rapid incident resolution 300-level live demo","tabs":["Demo preparation","Demo script"]},"exports":{},"rawBody":"---\ntitle: Proactively Assure Application Performance\n  300-level live demo\ndescription: Rapid incident resolution 300-level live demo\ntabs: [ 'Demo preparation', 'Demo script']\n---\n\nexport const Title = () => (\n  <span>\n    Proactively Assure Application Performance <br/> 300-level live demo\n  </span> );\n\n![banner](./images/Incident-Resolution-300-Script.jpg)\n\n<span id=\"place1\"></span>\n\n<details>\n<summary>Introduction</summary>\n<br/>\n\nIn this demo we will:<br/><br/>First, use Instana to discover the various business applications and get a quick view of their overall health.<br/><br/>Then, we’ll see how Turbonomic leverages the information from Instana and other workload controllers to provide recommendations for application performance & resource optimization.<br/><br/>Lastly, we’ll see how Turbonomic recommendations can be ***automatically*** executed via defined automation policies that effectively remove the need for human intervention. After the requisite level of TRUST is established, these policies enable the system to proactively take remedial actions when appropriate.\n<br/>\n(Printer-ready PDF of demo script <a href=\"./NEW PDF HERE.pdf\" target=\"_blank\" rel=\"noreferrer\">here</a>)\n\n<br/><br/>\n\n**[Go to top](#place1)**\n\n</details>\n\n<details>\n\n<summary>1 - Visualizing your environment</summary>\n\n<br/>\n\n| **1.1** | **Navigate to the Anomaly Generator and input sources** |\n| :--- | :--- |\n| **Actions** | Navigate to **Instana** <br/><br/>Click **Applications** in left-hand sidebar menu |\n| **Narration** | We’re a retail company with multiple online storefronts representing each of our diversified brands. For example, we sell robots via our Robot Shop-branded ecommerce site.<br/><br/>**This is a list of all our business applications, which we are monitoring with Instana. Instana provides simplicity with its full-stack discoverability capabilities and an ability to trace every request end-to-end. This makes it easy to track each application’s health and performance. We can quickly see key information for each business application, including number of calls, latency, and the erroneous call rate.<br/><br/>Some users have been complaining about slowness and unpredictable performance issues with the Robot Shop Application recently. |\n| **Screenshot** | <br/> ![IMAGE PENDING](./images/quote-app.png) |\n\n| **1.2** | **Verify which hosts the app runs on** |\n| :--- | :--- |\n| **Actions** | Click on **Robot Shop**<br/><br/>Then click the **Stack** button to see the dropdown<br/><br/>Click **Infrastructure** to see the hosts that Robot Shop runs on |\n| **Narration** | To accelerate our strategic digital transformation initiatives, our company has embraced cloud native architecture. Applications are build based on microservices and deployed to a Kubernetes-based infrastructure. Many lines of business deploy to and share a common infrastructure. This means that a node might have parts of multiple and different business applications on it. When one application suddenly has an increase in workload, it can affect the others as well. Since not all applications are built reliably, some may have unpredictable load patterns - especially during promotion events.<br/><br/>For example, our Robot Shop application has components running on two different nodes or “hosts” as they are known in Instana. |\n| **Screenshot** | <br/> ![IMAGE PENDING](./images/quote-app.png) |\n\n| **1.3** | **Shared Deployment Infrastructure** |\n| :--- | :--- |\n| **Actions** | Click on the **worker2** host<br/><br/>On worker2’s summary screen, click the **Stack** button to see the dropdown<br/><br/>Click **Application** to see the other applications with components on worker2 |\n| **Narration** | Looking at one of the hosts, worker2, we can see that it has components of four different applications running on it. It’s easy to see that if there’s a sudden or unexpected increase in demand on Robot Shop, it could affect the Quote of the Daya app as well - or vice versa. |\n| **Screenshot** | <br/> ![IMAGE PENDING](./images/quote-app.png) |\n\n<br/><br/>\n\n**[Go to top](#place1)**\n\n</details>\n\n<details>\n\n<summary>2 - Gaining full stack visibility into your hybrid and multicloud environments</summary>\n\n<br/>\n\n| **2.1** | **Get the big picture** |\n| :--- | :--- |\n| **Action** | Navigate to the **Turbonomic** Home Page – **Global View**<br/><br/>Examine **Global Supply Chain**<br/><br/>Hoover over the **Supply Chain**<br/><br/>Click **Virtual Machines Entity** in Supply Chain |\n| **Narration** | Before we take a deeper look at the Robot Shop app specifically, let’s take a step back and understand our application infrastructure more wholistically. Older applications were deployed to isolated environments, but modern apps are deployed to a highly shared, increasingly dense virtualized hybrid and multi-cloud environment. Hence getting a complete view is essential as resourcing decisions in one area will naturally impact another.<br/><br/>Turbonomic ingests configuration and operational data from various targets like Instana and combines all the information it has discovered into a common data model and represents it as a Supply Chain.<br/><br/>The Supply Chain provides the big picture, serving as an informative graphical organizer of the various entities in the IT estate. It also shows the implicit dynamic relationships, all the way from the application to its underlying infrastructure dependencies.<br/><br/>This quick visualization provids incredibly valuable information to the IT Ops teams. It helps to bridge the communication gap that often exists between the application owners and the infrastructure teams.<br/><br/>Okay, let’s explore the Supply Chain in a bit more detail here:<br/><br/>• Starting right at the top you have the Business Applications. Note this environment consists of 63 business applications<br/>• There are 544 key business transactions and 1000 services that are part of this estate.<br/>• The top three layers are called LOGICAL layers – they do not consume physical resources<br/>• The services are deployed to physical resources like JVMs or Operating Systems processes that run in a container or VM.<br/>• Going down the Supply Chain you can see the components that build up the lower layers of the application stack – including Database Servers, Storage Volumes and Hosts among others.<br/>• Going even further down the stack you can see how the virtualized resources could be part of. Virtual Data Center or part of a public Cloud region and availability zone.<br/>• Each circle in the supply chain represents a logical or physical entity.<br/>• The number inside is the count of the number of entities of this type<br/>• The ring of each entry indicates the percentage of pending actions<br/>• Green shows percentage of entities that have NO pending actions<br/>• Yellow shows entities that have efficiency pending actions<br/>• Red shows entities that have critical performance pending actions<br/>• Hover over the entity type to get actual counts of pending actions<br/>• The arrow from one entry to another indicates the flow of resource consumption<br/>• For example –a VM consumes resources from hosts and storage<br/>• You can see this environment consists of 972 Virtual Machines.<br/>• Clicking on an Entity Type will bring you to a detailed view into those entities.<br/><br/>Internally, the supply chain model is key to the optimization analytics that Turbonomic performs on the environment. It models each entity in the environment as part of an economic model – where some resources are buyers, and some are sellers. Each resource is allocated a budget and Turbonomic continually assesses the supply and demand of resources. If something is in short supply, its price goes up. This dynamic adjustment using a market-based approach means that Turbonomic can find the most efficient use of resources given performance needs and constraints. |\n| **Screenshot** | <br/> ![IMAGE TO COME](./images/incident-title.png) |\n<br/><br/>\n\n**[Go to top](#place1)**\n\n</details>\n\n\nCURRENT\n\n<details>\n\n<summary>3 - SCOPE the analysis to Robot Shop</summary>\n\n<br/>\n\n| **3.1** | **SCOPE to the Robot Shop Supply Chain** |\n| :--- | :--- |\n| **Action** | Scroll to the **Relevant events** section of the story |\n| **Narration** | We need to find out where the issue began so we can prevent it from causing cascading failures across the components of the application.<br/><br/>Instead of having to go to Prometheus or another tool to look at the alerts, we can see them right here from the notification. Watson AIOps has determined that these events are related, and provides an explanation for how it determined the relationships. We can see that there are two groups of events based on related resources and the timing of the events.<br/><br/>\n| **Screenshot** | <br/> ![Relevant events](./images/relevant-events.png) |\n| **Actions** | Click on the **View relevant events** button at the bottom of the notification<br/><br/>A pop-up will appear with the grouped events<br/>\n| **Narration** | We can inspect the grouped events right here, without searching for them in another tool.<br/><br/>It looks like the memory and CPU on the Rating service increased significantly. This is causing a significant slow-down in response time on both the Rating and Web services.<br/><br/>Based on this information, it seems that the Rating service is the source of the issue. But let’s get a bit more detail – this time from the log files.<br/><br/>\n| **Screenshots** | <br/> ![View relevant events](./images/view-relevant-events.png) <br/>![Relevant events alerts](./images/relevant-events-alerts.png) |\n| **Action** | Scroll down past the alerts to show **Log anomaly** |\n| **Narration** | Instead of needing to go to Kibana and manually sort through the hundreds or thousands of log entries that come in every minute, Watson AIOps has found several anomalies in the log files and presented them here. It trains on the log files of the application when it’s operating normally, and continually monitors for deviations from that baseline.<br/><br/>We can see that the anomalies are occurring on the Rating service, which fits with what we saw in the alerts. |\n| **Screenshot** | <br/> ![Rating service](./images/rating-service.png) |\n| **Action** | Click on **Show more** |\n| **Narration** | Watson AIOps gives us additional context on the anomaly. In this case, the “unknown_error” anomaly is telling us that Watson AIOps has never seen this type of log before (hence the “unknown”) and that the log message indicates there is some type of error. Watson AIOps is not only looking at the statistical frequency of the type of log, but it is also using Natural Language Processing to analyze the content of the log message to give additional context (in this case that there’s likely an error).<br/><br/>Watson AIOps also explains why the anomaly was flagged. It expected to see zero (0) of this type of log, but it actually saw four (4).<br/><br/>Now we know that there is an unfamiliar log coming from the Rating service, and it’s indicating an error.<br/><br/>This further reinforces what we saw with the alerts -  it looks like the Rating service is likely the root cause of this problem.<br/><br/>\n| **Screenshots** | <br/> ![anomaly](./images/anomaly.png) <br/>![anomaly 2](./images/log-anomaly-1.png) |\n| **Action** | For the 'Unknown_error' anomaly, click on **Preview logs** |\n| **Narration** | We can preview the log messages that caused Watson AIOps to find the anomaly. |\n| **Screenshots** | <br/> ![Preview logs](./images/preview-logs.png) <br/> ![Log preview](./images/log-preview.png) |\n\n**[Go to top](#place1)**\n\n</details>\n\n<details>\n\n<summary>4 - Towards self-healing and automating your action FINALIZE</summary>\n\n<br/>\n\n| **4.1** | **Watson AIOps searches similar incidents for recommendations** |\n| :--- | :--- |\n| **Actions** | Close the log anomaly pop-up<br/><br/>Click on the **Search similar incidents** button<br/><br/>Type 'rating' into the search box |\n| **Narration** | Now that we understand a bit more about what’s going on, we need to focus on our main goal: resolving the incident as quickly as possible.<br/><br/>Watson AIOps also brings in similar incidents and information regarding how they were resolved. This enhances operational efficiency by leveraging institutional knowledge that may be time consuming to find otherwise.<br/><br/>Since the alerts seem to point to the Rating service as having the memory and CPU increases, we’ll search for incidents that happened with that service. <br/><br/>Watson AIOps found two similar incidents. The first one seems like what we’re experiencing: Rating service is overheating and slowing. Using Natural Language Processing, Watson AIOps  went through the comments on the incident and highlighted key relevant information for us, in this case that the incident was resolved by running a runbook.<br/><br/>If we can use the same runbook to resolve the current incident it will make our job that much easier and restore service even faster! |\n| **Screenshots** | <br/> ![Search similar incidents](./images/search-similar-incidents.png) <br/>![Search similar incidents - Rating](./images/search-similar-incidents-rating.png) <br/>![Search results](./images/search-results.png) |\n| **Actions** | Click on the link for the first similar incident **Rating service overheating and slowing**<br/><br/>It will open a window in GitLab<br/><br/>If needed, sort comments **newest first** |\n| **Narration** | The description for this incident sounds a lot like what we are observing right now. The Rating service is stressed, with increases in CPU, memory, and latency. It’s also affecting the Web service.<br/><br/>The comment says there is a runbook that resolved this issue. This is very useful information that may have taken a long time to find out just by searching or asking colleagues. Watson AIOps helped us find this info very quickly. |\n| **Screenshots** | <br/> ![Newest first](./images/newest-first.png) |\n\n**[Go to top](#place1)**\n\n</details>\n\n<details>\n\n<summary>5 - Fixing the problem and restoring service</summary>\n\n<br/>\n\n| **5.1** | **View Event Manager to determine the root cause and fix the problem** |\n| :--- | :--- |\n| **Actions** | Navigate to **Event Manager** (formally known as NOI)<br/><br/>Click on **Events** in the sidebar menu<br/><br/>The top event should show a red circle (**critical**)<br/><br/>Click on the **Down** arrow to expand it |\n| **Narration** | This is the Event Manager component of Watson AIOps. Here’s the event group that we’re working on. There are four events grouped together – these are the same ones we previewed earlier from the Slack notification. There’s additional detail here, including the likelihood of each event being the root cause of the overall issue and if there is a runbook associated with the event.<br/><br/>There are three events that occurred on the Rating service. They’re all rated as 99% likely to be the root cause - meaning that the Rating service is the source of the problem.<br/><br/>It also shows us that there’s a runbook associated with these events, which is what was mentioned in the GitLab incident comment. |\n| **Screenshots** | <br/> ![Events1](./images/events1.png) <br/> ![Events2](./images/events2.png) |\n| **Actions** | Click on the second row to select it (**Summary** column: **Rating service Mem usage very high**)<br/><br/>Click on **Execute runbook**<br/><br/>Click on **Start runbook**<br/><br/>Click on **Run** |\n| **Narration** | Now we’ll execute the runbook that should resolve the underlying failure of the Rating service. That should fix the problem. |\n| **Screenshots** | <br/> ![Events3](./images/events3.png) <br/>![runbook1](./images/runbook1.png) <br/> ![Runbook2](./images/runbook2.png) |\n| **Actions** | Navigate to **Instana**<br/><br/>Click on the **Summary** tab<br/><br/>Set the time period to **last 15 minutes**, and click on the **Live** button (you will be showing the latency before, during, and after the incident) |\n| **Narration** | To check that the runbook is fixing the problem, we can look at the health metrics in Instana. We’ll set the time period so we can see data from when the incident began until now.<br/><br/>We can see that the latency of the application started increasing when the problem began. Now that we’ve run the runbook, latency is going back down. This confirms that the runbook was successful. |\n| **Screenshot** | <br/> ![runbook success](./images/runbook-success.png) |\n\n**[Go to top](#place1)**\n\n</details>\n\n<details>\n\n<summary>Summary</summary>\n\n<br/>\nUsing Turbonomic and Instana, we were able to easily identify changes needed to maintain quality of service, as well as improve efficiency, for our multiple applications which included components running on AWS and OpenShift.\n\nFirst, using Instana, we were able to discover the business applications and monitor their health.\n\nThen, using Turbonomic, we got specific recommendations on how to assure performance and improve efficiency of our operations.\n<br/><br/>\n\n**[Go to top](#place1)**\n\n</details>\n","fileAbsolutePath":"/home/runner/work/platinum-demos/platinum-demos/src/pages/300-watson-aiops-proactively-assure-application-performance/demo-script.mdx"}}},"staticQueryHashes":["1364590287","137577622","137577622","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}